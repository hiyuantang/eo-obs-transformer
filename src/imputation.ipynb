{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e06c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original data from: ./data\\senegal_shuffled.csv\n",
      "Loaded 2165 records.\n",
      "Generating correct UIDs based on within-year index...\n",
      "UID generation complete.\n",
      "Sample generated UIDs:\n",
      "   country  year  group_index                 uid\n",
      "0  senegal  1992            0  senegal_1992_00000\n",
      "1  senegal  1992            1  senegal_1992_00001\n",
      "2  senegal  1992            2  senegal_1992_00002\n",
      "3  senegal  1992            3  senegal_1992_00003\n",
      "4  senegal  1992            4  senegal_1992_00004\n",
      "Processing TFRecords and imputing coordinates for 2165 locations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6e2cb8711e4f34b6d2439bcfb74f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging imputed coordinates back into the original DataFrame...\n",
      "Creating origin_lat/origin_lon columns...\n",
      "Updating lat/lon columns with imputed values where successful...\n",
      "Updated lat/lon for 462 locations.\n",
      "\n",
      "--- Imputation Summary ---\n",
      "Successfully imputed coordinates for: 462 locations\n",
      "Failures due to missing TFRecord:   1565\n",
      "Failures due to no NL signal in radius: 138\n",
      "Failures due to other errors:       0\n",
      "Dropping temporary imputation columns...\n",
      "Saving imputed DataFrame to: ./data\\senegal_shuffled_coords_imputed_2.csv\n",
      "\n",
      "Imputation script finished.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "# --- Configuration ---\n",
    "NL_DATA_BASE_DIR = \"./downloaded_gcs_data/senegal_NL/\" \n",
    "ORIGINAL_CSV_PATH = os.path.join('./data', 'senegal_shuffled.csv')\n",
    "OUTPUT_CSV_PATH = os.path.join('./data', 'senegal_shuffled_coords_imputed_2.csv') \n",
    "\n",
    "# Image dimensions (CONFIRMED 384x384)\n",
    "IMG_HEIGHT = 384\n",
    "IMG_WIDTH = 384\n",
    "EXPECTED_PIXELS = IMG_HEIGHT * IMG_WIDTH\n",
    "PIXEL_SCALE = 30 \n",
    "\n",
    "# Displacement Radii (meters)\n",
    "RADIUS_METERS_URBAN = 2000\n",
    "RADIUS_METERS_RURAL = 5000\n",
    "\n",
    "# Night Light Threshold for binary likelihood mask (Prior P(x,y|NL))\n",
    "NL_THRESHOLD = 0.05 \n",
    "\n",
    "# --- Helper Function: Coordinate Approximation ---\n",
    "def displace_lat_lon(lat, lon, dx_meters, dy_meters):\n",
    "    R_EARTH = 6378137\n",
    "    m_per_deg_lat = 111132.954 - 559.822 * math.cos(2 * math.radians(lat)) + 1.175 * math.cos(4 * math.radians(lat))\n",
    "    delta_lat = dy_meters / m_per_deg_lat\n",
    "    new_lat = lat + delta_lat\n",
    "    m_per_deg_lon = (math.pi / 180) * R_EARTH * math.cos(math.radians(lat))\n",
    "    if abs(m_per_deg_lon) < 1e-6: delta_lon = 0\n",
    "    else: delta_lon = dx_meters / m_per_deg_lon\n",
    "    new_lon = lon + delta_lon\n",
    "    return new_lat, new_lon\n",
    "\n",
    "# --- Main Processing Logic ---\n",
    "\n",
    "# 1. Load original DataFrame\n",
    "print(f\"Loading original data from: {ORIGINAL_CSV_PATH}\")\n",
    "try:\n",
    "    df_orig = pd.read_csv(ORIGINAL_CSV_PATH)\n",
    "    if 'rural' not in df_orig.columns:\n",
    "        print(\"Error: 'rural' column not found.\")\n",
    "        exit()\n",
    "    print(f\"Loaded {len(df_orig)} records.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Original CSV file not found at {ORIGINAL_CSV_PATH}\")\n",
    "    exit()\n",
    "except KeyError:\n",
    "     print(\"Error: CSV seems to be missing expected columns.\")\n",
    "     exit()\n",
    "\n",
    "# 2. Generate CORRECT UID based on within-year index\n",
    "print(\"Generating correct UIDs based on within-year index...\")\n",
    "df_orig['group_index'] = df_orig.groupby(['country', 'year']).cumcount()\n",
    "df_orig['uid'] = df_orig.apply(\n",
    "    lambda row: f\"{row['country']}_{row['year']}_{int(row['group_index']):05d}\",\n",
    "    axis=1\n",
    ")\n",
    "print(\"UID generation complete.\")\n",
    "print(\"Sample generated UIDs:\")\n",
    "print(df_orig[['country', 'year', 'group_index', 'uid']].head())\n",
    "\n",
    "\n",
    "# 3. Prepare results storage\n",
    "results = []\n",
    "\n",
    "# 4. Loop through DataFrame and Process TFRecords\n",
    "print(f\"Processing TFRecords and imputing coordinates for {len(df_orig)} locations...\")\n",
    "center_row, center_col = IMG_HEIGHT // 2, IMG_WIDTH // 2\n",
    "y_coords, x_coords = np.ogrid[:IMG_HEIGHT, :IMG_WIDTH]\n",
    "dist_from_center = np.sqrt((x_coords - center_col)**2 + (y_coords - center_row)**2)\n",
    "dist_from_center[center_row, center_col] = 1.0 \n",
    "\n",
    "for index, row in tqdm(df_orig.iterrows(), total=len(df_orig)):\n",
    "    uid = row['uid']\n",
    "    country = row['country']\n",
    "    year = row['year'] \n",
    "    is_rural = row['rural']\n",
    "    orig_lat, orig_lon = row['lat'], row['lon']\n",
    "\n",
    "    location_result = {\n",
    "        'uid': uid, 'imputed_lat': orig_lat, 'imputed_lon': orig_lon,\n",
    "        'imputation_success': False, 'imputation_error': None\n",
    "    }\n",
    "    tfrecord_path = os.path.join(NL_DATA_BASE_DIR, f\"{country}_{year}\", f\"{uid}.tfrecord.gz\")\n",
    "\n",
    "    if not os.path.exists(tfrecord_path):\n",
    "        location_result['imputation_error'] = 'TFRecord file not found'\n",
    "        results.append(location_result)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # --- Load NL Patch ---\n",
    "        raw_dataset = tf.data.TFRecordDataset(tfrecord_path, compression_type='GZIP')\n",
    "        serialized_example = next(iter(raw_dataset))\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(serialized_example.numpy())\n",
    "        feature_map = example.features.feature\n",
    "\n",
    "        nl_image_patch = None\n",
    "        if len(feature_map) == 1:\n",
    "            feature_key = list(feature_map.keys())[0]\n",
    "            feature = feature_map[feature_key]\n",
    "            if feature.float_list.value and len(feature.float_list.value) == EXPECTED_PIXELS:\n",
    "                nl_image_patch = np.array(feature.float_list.value, dtype=np.float32).reshape((IMG_HEIGHT, IMG_WIDTH))\n",
    "            elif not feature.float_list.value:\n",
    "                 nl_image_patch = np.full((IMG_HEIGHT, IMG_WIDTH), np.nan, dtype=np.float32)\n",
    "                 location_result['imputation_error'] = 'TFRecord feature empty'\n",
    "            else:\n",
    "                location_result['imputation_error'] = 'Pixel length mismatch'\n",
    "        else:\n",
    "            location_result['imputation_error'] = f'Found {len(feature_map)} features, expected 1'\n",
    "\n",
    "        if nl_image_patch is None:\n",
    "            results.append(location_result)\n",
    "            continue\n",
    "\n",
    "        # --- Create Prior based on NL (P(x,y | NL)) ---\n",
    "        prior_map_nl = (nl_image_patch > NL_THRESHOLD).astype(np.float32)\n",
    "        prior_map_nl[np.isnan(nl_image_patch)] = 0.0\n",
    "\n",
    "        # --- Create Likelihood based on Displacement (P(x',y' | x,y) ~ 1/d) ---\n",
    "        radius_meters = RADIUS_METERS_RURAL if is_rural == 1 else RADIUS_METERS_URBAN\n",
    "        radius_pixels = radius_meters / PIXEL_SCALE\n",
    "        likelihood_map_displacement = np.zeros_like(dist_from_center, dtype=np.float32)\n",
    "        mask_within_radius = dist_from_center <= radius_pixels\n",
    "        likelihood_map_displacement[mask_within_radius] = 1.0 / dist_from_center[mask_within_radius]\n",
    "\n",
    "        # --- Calculate Posterior Probability Map ---\n",
    "        posterior_map = likelihood_map_displacement * prior_map_nl\n",
    "\n",
    "        # --- Sample from Posterior ---\n",
    "        posterior_sum = np.sum(posterior_map)\n",
    "        if posterior_sum > 1e-9:\n",
    "            probabilities = posterior_map / posterior_sum\n",
    "            flat_probs = probabilities.flatten()\n",
    "            flat_probs = np.maximum(flat_probs, 0) \n",
    "            flat_probs /= np.sum(flat_probs) \n",
    "\n",
    "            flat_indices = np.arange(len(flat_probs))\n",
    "            chosen_flat_index = np.random.choice(flat_indices, p=flat_probs)\n",
    "            chosen_row, chosen_col = np.unravel_index(chosen_flat_index, (IMG_HEIGHT, IMG_WIDTH))\n",
    "\n",
    "            dx_pixels = chosen_col - center_col\n",
    "            dy_pixels = center_row - chosen_row\n",
    "            dx_meters = dx_pixels * PIXEL_SCALE\n",
    "            dy_meters = dy_pixels * PIXEL_SCALE\n",
    "\n",
    "            imputed_lat, imputed_lon = displace_lat_lon(orig_lat, orig_lon, dx_meters, dy_meters)\n",
    "\n",
    "            location_result['imputed_lat'] = imputed_lat\n",
    "            location_result['imputed_lon'] = imputed_lon\n",
    "            location_result['imputation_success'] = True\n",
    "        else:\n",
    "            location_result['imputation_error'] = 'No valid settlement pixels found within radius (weighted)'\n",
    "\n",
    "    except StopIteration:\n",
    "        location_result['imputation_error'] = 'TFRecord file is empty'\n",
    "    except Exception as e:\n",
    "        location_result['imputation_error'] = f'Processing error: {str(e)}'\n",
    "    results.append(location_result)\n",
    "\n",
    "# 5. Create DataFrame from results and Merge\n",
    "print(\"\\nMerging imputed coordinates back into the original DataFrame...\")\n",
    "df_results = pd.DataFrame(results)\n",
    "merge_cols = ['uid', 'imputed_lat', 'imputed_lon', 'imputation_success', 'imputation_error']\n",
    "df_output = pd.merge(df_orig, df_results[merge_cols], on='uid', how='left')\n",
    "df_output = df_output.drop(columns=['group_index'])\n",
    "\n",
    "# 5a. Preserve original coordinates\n",
    "print(\"Creating origin_lat/origin_lon columns...\")\n",
    "df_output['origin_lat'] = df_output['lat']\n",
    "df_output['origin_lon'] = df_output['lon']\n",
    "\n",
    "# 5b. Update lat/lon with imputed values where successful\n",
    "print(\"Updating lat/lon columns with imputed values where successful...\")\n",
    "success_mask = (df_output['imputation_success'] == True) & \\\n",
    "               (df_output['imputed_lat'].notna()) & \\\n",
    "               (df_output['imputed_lon'].notna())\n",
    "\n",
    "df_output.loc[success_mask, 'lat'] = df_output.loc[success_mask, 'imputed_lat']\n",
    "df_output.loc[success_mask, 'lon'] = df_output.loc[success_mask, 'imputed_lon']\n",
    "print(f\"Updated lat/lon for {success_mask.sum()} locations.\")\n",
    "\n",
    "# 7. Report Summary\n",
    "print(\"\\n--- Imputation Summary ---\")\n",
    "num_success = df_output['imputation_success'].sum() \n",
    "num_fail_file = df_output['imputation_error'].str.contains('not found', na=False).sum() \n",
    "num_fail_nolight = df_output['imputation_error'].str.contains('No valid settlement', na=False).sum() \n",
    "num_fail_other = df_output['imputation_error'].notna().sum() - num_fail_file - num_fail_nolight \n",
    "\n",
    "print(f\"Successfully imputed coordinates for: {num_success} locations\")\n",
    "print(f\"Failures due to missing TFRecord:   {num_fail_file}\")\n",
    "print(f\"Failures due to no NL signal in radius: {num_fail_nolight}\")\n",
    "print(f\"Failures due to other errors:       {num_fail_other}\")\n",
    "if num_fail_other > 0:\n",
    "    print(\"Sample other errors:\")\n",
    "    other_error_mask = (df_output['imputation_success'] == False) & \\\n",
    "                       df_output['imputation_error'].notna() & \\\n",
    "                       ~df_output['imputation_error'].str.contains('not found|No valid settlement', na=True)\n",
    "    print(df_output.loc[other_error_mask, ['uid', 'imputation_error']].head())\n",
    "\n",
    "# 5c. Drop temporary imputation-related columns\n",
    "print(\"Dropping temporary imputation columns...\")\n",
    "cols_to_drop = ['imputed_lat', 'imputed_lon', 'imputation_success', 'imputation_error']\n",
    "cols_to_drop_existing = [col for col in cols_to_drop if col in df_output.columns]\n",
    "df_output.drop(columns=cols_to_drop_existing, inplace=True)\n",
    "\n",
    "# 6. Save the output CSV\n",
    "print(f\"Saving imputed DataFrame to: {OUTPUT_CSV_PATH}\")\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV_PATH), exist_ok=True)\n",
    "df_output.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "\n",
    "print(\"\\nImputation script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat288",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
